{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LXk-ncK4uV4"
   },
   "source": [
    "# Strojový překlad pomocí rekurentních neuronových sítí\n",
    "\n",
    "Vyzkoušíme si strojový překlad mezi češtinou a angličtinou pomocí rekurentních neuronových sítí. K tomu použijeme model encoder-decoder, což jsou vlastně 2 RNN, které už jsme probírali a známe. Encoder čte vstupní sekvenci a vydá jeden vektor decoderu, který z něj vytvoří vstupní sekvenci. To funguje tak, že na vstupu máme větu, kterou rozsekáme na slova a každé slovo předáme encoderu, který mu dá embedding podle kontextu ostatních slov ve větě, kde si každé slovo bere informaci od předchozího a toho, co ten předchozí až do té doby zjistil. Dekodér je další RNN, která vezme výstupní vektor encoderu a vytvoří sekvenci slov, která je překladem původního vstupu a to tak, že se kouká doleva na to, co už bylo přeloženo a navíc si bere kontextový vektor z encoderu.\n",
    "\n",
    "K natrénování si tohoto modelu použijeme [Paralelní korpus Europarl](https://www.statmt.org/europarl/), který byl získán z jednání Evropského parlamentu a obsahuje dvojjazyčné zápisy z jednání v angličtině a dalších evropských jazycích. Mezi nimi je i čestina, kterou požijeme pro náš překladač. Jak už je zřejmé z povahy dat, bude náš překladač schopný přeložit jen texty, které budou mít stejnou povahu jako zdrojová data, nebude to tedy univerzální překladač, ale tzv. domain-based, což znamená, že bude umět pracovat jen v určité doméně neboli oblasti -- v našem případě jednání Evropského parlametu. Tento tutoriál je inspirovaný [odsud](https://github.com/Hvass-Labs/TensorFlow-Tutorials#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "al19Ni7F6Dgg",
    "outputId": "b3369452-6ef3-4c3a-960e-fdfde234949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katie\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiQFGTUE_vKC"
   },
   "source": [
    "Nejprve si načteme data a (pokud je nemáme stažená, tak si je stáheneme) vyrobíme si také symboly pro označení začátku a konce sekvence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "language_code='cs'\n",
    "\n",
    "def load_data(english=True, language_code=\"da\", start=\"\", end=\"\"):\n",
    "    data_dir='data'\n",
    "    if english:\n",
    "        filename = \"europarl-v7.{0}-en.en\".format(language_code)\n",
    "    else:\n",
    "        filename = \"europarl-v7.{0}-en.{0}\".format(language_code)  \n",
    "    path = os.path.join(data_dir, filename)\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        texts = [start + line.strip() + end for line in file]\n",
    "    return texts\n",
    "\n",
    "data_src = load_data(english=False,language_code=language_code)\n",
    "data_dest = load_data(english=True,language_code=language_code,start=mark_start,end=mark_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rO5TR2h6JXb"
   },
   "source": [
    "Když máme nahraná data, tak uděláme jejich preprocessing. K tomu použijeme třídu ```Tokenizer```, která rozseká texty na slova, vytvořit slovník unikátních slov, kterým přiřadíme čísla, a potom každé slovo v datech nahradit jeho číselnou hodnotou. Navíc mohou mít sekvence různé délky, takže je potřeba nastavit všem stejnou, tedy některé zkrátit a jiné dopaddovat. Délku nastavíme tak, aby bylo potřeba zkrátit je asi 5% sekvencí, což je poměrně dobrý kompromis vzhledem k velikosti paměti a zachování dat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC_F-TjsQ8U9"
   },
   "outputs": [],
   "source": [
    "# Tokenizer k nahrazení slov číselnou hodnotou\n",
    "num_words = 10000\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Vytvoření slovníku z textu\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Vytvoření inverzního vyhledávání od celočíselných tokenů ke slovům.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),self.word_index.keys()))\n",
    "\n",
    "        # Převod všech textů na seznam celočíselných tokenů\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Obrácení sekvenece tokenů\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            # Příliš dlouhé obrácené sekvence by měly být zkráceny na začátku, což odpovídá konci původních.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Příliš dlouhé sekvence by měly být zkráceny na konci.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # Zjištění počtu celočíselných tokenů v každé sekvenci\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Doplnění nebo zkrácení sekvencí na stejnou délku.\n",
    "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        self.tokens_padded = pad_sequences(self.tokens, maxlen=self.max_tokens, padding=padding, truncating=truncating)\n",
    "    \n",
    "    # Vyhledání slova z celočíselného tokenu\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    # Převod seznamu celočíselných tokenů na řetězec\n",
    "    def tokens_to_string(self, tokens):\n",
    "\n",
    "        # Vytvoření listu slov\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        \n",
    "        # Spojení slov do stringu pomocí mezery\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "\n",
    "    # Convert a single text-string to tokens with optional reversal and padding\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "\n",
    "        # Převod textu na tokeny\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Obrácení tokenu\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "\n",
    "            # Příliš dlouhé obrácené sekvence by měly být zkráceny na začátku, což odpovídá konci původních.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated at the end.\n",
    "            truncating = 'post'\n",
    "        if padding:\n",
    "            # Příliš dlouhé sekvence by měly být zkráceny na konci.\n",
    "            tokens = pad_sequences(tokens,  maxlen=self.max_tokens, padding='pre', truncating=truncating)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,padding='pre',reverse=True,num_words=num_words)\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest, padding='post', reverse=False,num_words=num_words)\n",
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_end = tokenizer_dest.word_index[mark_end.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw2Sg1hTAHlR"
   },
   "source": [
    "Nyní už jen data preprocessovaná do celočíselných tokenů vhodně předáme encoderu a decoderu, abychom mohli na trénovat naši neuronovou síť."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "66W7es_WAFn0",
    "outputId": "3a6ea424-11c8-40e5-9bec-5061e27b7092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646605, 52)\n",
      "(646605, 52)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = tokens_src\n",
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vi_Lc0Q_jP0"
   },
   "source": [
    "Pak už můžeme vytvořit neuronovou síť. Začneme tím, že si naprogramujeme encoder, který se bude skládat ze vstupní vrstvy, vrstvy embeddingů, 3 GRU vrstev a výstupu. Následovat bude decoder, který jako vstup bere výstup encoderu a bude se opět skládat ze vstupní vrsty, 3 GRU vrstev a výstupní vrstvy, která bude převádět výstup na one-hot zakódované pole. Ještě si definujeme ztrátovou funkci ```sparse_cross_entropy``` a pak můžeme model přeložit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "pcDzc4977eIt",
    "outputId": "87fadbd7-b17b-4ff3-f03f-2ecd74fc54f3"
   },
   "outputs": [],
   "source": [
    "# Vytvoření encoderu\n",
    "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
    "\n",
    "embedding_size = 128\n",
    "encoder_embedding = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')\n",
    "\n",
    "state_size = 512\n",
    "encoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False)\n",
    "\n",
    "# Spojení jednotlivých vrstev\n",
    "def connect_encoder(): \n",
    "    net = encoder_input\n",
    "    net = encoder_embedding(net)\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "    encoder_output = net  \n",
    "    return encoder_output\n",
    "\n",
    "encoder_output = connect_encoder()\n",
    "\n",
    "# Vytvoření decoderu\n",
    "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')\n",
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "decoder_embedding = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')\n",
    "decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True)\n",
    "decoder_dense = Dense(num_words, activation='linear', name='decoder_output')\n",
    "\n",
    "# Spojení jednotlivých vrstev\n",
    "def connect_decoder(initial_state):\n",
    "\n",
    "    net = decoder_input\n",
    "    net = decoder_embedding(net)\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output\n",
    "\n",
    "# Spojení a vytvoření modelu\n",
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output])\n",
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])\n",
    "\n",
    "# Ztrátová funkce\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean\n",
    "\n",
    "# Sestavení modelu\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
    "model_train.compile(optimizer=optimizer, loss=sparse_cross_entropy, target_tensors=[decoder_target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBJAb45PDpRG"
   },
   "source": [
    "Ještě se nám může hodit callback funkce pro zapisování checkpointů během trénování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SwetwghWDpbz",
    "outputId": "a26d4b41-400c-4cf2-dc54-60f1eb07b3bd"
   },
   "outputs": [],
   "source": [
    "# Callback function\n",
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss', verbose=1,\n",
    "                                      save_weights_only=True, save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/', histogram_freq=0, write_graph=False) \n",
    "callbacks = [callback_early_stopping, callback_checkpoint, callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTjY4cdMETyH"
   },
   "source": [
    "Nyní už se můžeme pustit do samotného trénovaní modelu na našich datech. V průběhu trénování si ukládáme po každé epoše checkpointy, takže po každé budeme mít k dispozici natrénované váhy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = { 'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data }\n",
    "y_data = { 'decoder_output': decoder_output_data }\n",
    " \n",
    "validation_split = 10000 / len(encoder_input_data)\n",
    "\n",
    "model_train.fit(x=x_data, y=y_data, batch_size=512, epochs=10, validation_split=validation_split, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvauaI_JEUYI"
   },
   "source": [
    "Model na CPU trénovat nebudeme, protože to trvá velice dlouho. Trénování jsem ale pustila za vás na GPU a natrénované váhy najdete přiložené k tomuto cvičení. K samotnému překladu testovacích vět si ještě napíšeme funkci ```translate```, která nám větu preprocessuje a předá ji síti a nechá ji přeložit. To uděláme tak, že vstupní text převedeme na sekvenci tokenů integerů a použijeme výstupní vrstvu encoderu, která bude použita jako počáteční stav v GRU dekodéru. Mohli bychom použít i koncový stav encoderu, ale to je skutečně nutné pouze tehdy, pokud encoder a decoder používají LSTM místo GRU, protože LSTM má dva vnitřní stavy. \n",
    "\n",
    "Potom dokud nenarazíme na konec sekvence nebo maximální počet tokenů budeme opakovat aktualizaci vstupní sekvence decoderu posledním vzorkovaným tokenem, v první iteraci nastavíme token na startovní. Pak si data zabalíme do slovníku pro přehlednost a bezpečnost,že jsme je zapsali ve správném pořadí a předáme je decoderu, aby nám predikoval překlad. Tokeny potom převeme na slova pomocí slovníku a ty sloučíme pomocí mezer do jednoho stringu a ten si vypíšeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnkOvxgeEUhq"
   },
   "outputs": [],
   "source": [
    "# Překlad jednoho stringu textu\n",
    "def translate(input_text, true_output_text=None):\n",
    "    # Převod vstupního textu do tokenů integerů\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text, reverse=True, padding=True)\n",
    "    \n",
    "    # Získání výstupní vrstvy encoderu\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Nastavení maximálního počtu tokenů v sekcenci\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    # Prealokace 2D pole pro vstup decoderu \n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # První vstupní token musí být 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Inicializace prázdného výstupního textu\n",
    "    output_text = ''\n",
    "\n",
    "    # Inicializace počtu už zpracovaných tokenů\n",
    "    count_tokens = 0\n",
    "\n",
    "    # Dokud není splněno ukočnovací kritérium\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "\n",
    "        # Update vstupní sekvence pro decoder\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Zabalení vstupních dat do slovníku \n",
    "        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data }\n",
    "\n",
    "        # Předání dat decoderu a predikce\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Získání posledního predikovaného tokenu jako one-hot pole\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # Převod na pole integerů\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # VYhledání slova odpovídající tokenu\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Přidání do výstupního textu\n",
    "        output_text += \" \" + sampled_word\n",
    "            \n",
    "        # Zvýšení counteru počtu tokenů\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sekvence tokenů vrácená decoderem\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    # Výpisy originální věty a překladu bez symbolů začátku a konce\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    print(\"Translated text:\")\n",
    "    if output_text[0]==' ':\n",
    "        output_text = output_text[1:]\n",
    "    if 'ssss' in output_text:\n",
    "        output_text = output_text[5:]\n",
    "    if 'eeee' in output_text:\n",
    "        output_text = output_text[:-5]\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Volitelná možnost výpisu skutečného překladu\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        if 'ssss' in true_output_text:\n",
    "            true_output_text = true_output_text[5:]\n",
    "        if 'eeee' in true_output_text:\n",
    "            true_output_text = true_output_text[:-5]\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ms8gkhHFJPtb"
   },
   "source": [
    "Zkusíme si tedy načíst natrénované váhy modelu a otestovat si ho pomocí překladu nějakých vět. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfCnoNp-TN5G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n"
     ]
    }
   ],
   "source": [
    "# Načtění checkpointu neboli vah do našeho modelu\n",
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "    print('Weights loaded')\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve se podíváme na překlad několika náhodně zvolených vět, na kterých jsme model trénovali a zároveň si i porovnáme překlad s původní větou. Všimněme si, že překladač vrací výsledek bez rozlišení velikosti písmen. To proto, že vstupní a výstupní texty jsou před trénováním lowercasované, aby to měl překladač snažší a nemusel rozlišovat ješt i velikosti písmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Texty smluv dodané Radou: viz zápis\n",
      "\n",
      "Translated text:\n",
      "texts of agreements forwarded by the council see minutes\n",
      "\n",
      "True output text:\n",
      "Texts of agreements forwarded by the Council: see Minutes\n",
      "\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "translate(input_text=data_src[idx], true_output_text=data_dest[idx])\n",
    "print('=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teď zkusíme přeložit i některé náhodné věty, které si sami vymyslíme. Nejprve se pokusíme vymyslet věty, které by mohly sedět do naší domény, tedy by mohly zaznít v jednání Evropského parlamentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Česká republika je členem Evropské Unie\n",
      "\n",
      "Translated text:\n",
      "the czech republic is a member of the european union\n",
      "\n",
      "True output text:\n",
      "The Czech Republic is a member of the European Union\n",
      "\n",
      "=====\n",
      "Input text:\n",
      "Vláda se rozhodla projednat nová pravidla pro cestování\n",
      "\n",
      "Translated text:\n",
      "the government has decided to discuss new rules on travel\n",
      "\n",
      "True output text:\n",
      "The government has decided to discuss new rules for travelling\n",
      "\n",
      "=====\n",
      "Input text:\n",
      "Francie tento návrh rozhodně nepodpoří\n",
      "\n",
      "Translated text:\n",
      "france has certainly not made it\n",
      "\n",
      "True output text:\n",
      "France will certainly not support this proposal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"Česká republika je členem Evropské Unie\", true_output_text='The Czech Republic is a member of the European Union')\n",
    "print('=====')\n",
    "translate(input_text=\"Vláda se rozhodla projednat nová pravidla pro cestování\", true_output_text='The government has decided to discuss new rules for travelling')\n",
    "print('=====')\n",
    "translate(input_text=\"Francie tento návrh rozhodně nepodpoří\", true_output_text='France will certainly not support this proposal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní si pro zajímavost zkusíme vymyslet nějaké věty, které určitě z naší domény nejsou, tedy s největší pravděpodobností v žádném jednání Evropského parlamentu nezazněla, i když jeden nikdy neví. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Dneska mě tu to fakt nebaví\n",
      "\n",
      "Translated text:\n",
      "i am surprised that this is a reality\n",
      "\n",
      "True output text:\n",
      "I really don't like it today\n",
      "\n",
      "=====\n",
      "Input text:\n",
      "Můj pes mi ukradl noviny\n",
      "\n",
      "Translated text:\n",
      "my dear colleagues\n",
      "\n",
      "True output text:\n",
      "My dog stole my newspaper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"Dneska mě tu to fakt nebaví\", true_output_text='I really don\\'t like it today')\n",
    "print('=====')\n",
    "translate(input_text=\"Můj pes mi ukradl noviny\", true_output_text='My dog stole my newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "translate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
