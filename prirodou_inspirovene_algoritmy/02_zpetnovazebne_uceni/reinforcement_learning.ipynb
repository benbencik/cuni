{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zpětnovazební učení"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto cvičení budeme pracovat s Open AI gym, což je open source rozhraní určené pro úkoly zpětnovazebního učení. Jeho hlavní výhodou je, že implementace různých typů algoritmů pro zpětnovazební učení je v něm celkem jednoduchá. Popis základních funkcí Open AI gym se nachází v kódu níž.\n",
    "\n",
    "Dnešní úkol bude naimplementovat agenta, který se učí chovat v nějakém prostředí (konkrétně v MountainCar) pomocí Q-učení.\n",
    "\n",
    "Q-učení je způsob, kdy se agent učí svou strategii, jak se chovat v daném prostředí, pomocí zpětné vazby, kterou od prostředí za své chování dostává. Na rozdíl od hladového agenta, který jen v každém stavu vybírá nový stav na základě akce, co maximalizuje jeho užitek, bere v potaz to, že mezi stavy existují vztahy, které jsou dány Bellmanovými rovnicemi.\n",
    "\n",
    "Nyní se tedy podíváme na příklad autíčka,které se snaží dostat do cíle, ale zatím se pohybuje náhodně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukážeme si, jak si vytvořit jednoduché prostředí *MountainCar*: https://gym.openai.com/envs/MountainCar-v0. \n",
    "\n",
    "Cílem je, aby se autíčko dostalo z údolí až nahoru k vlaječce, ale nemá dost silný motorek, takže se musí nejprve rozhoupat, aby tam vyjelo. V této základní verzi je zde v každém stavu náhodně zvolena akce pro pohyb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jen tak pro zajímavost existuje i například prostředí *CartPole*, kde je zase cílem vyvažovat tyčku, aby nespadla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\katie\\onedrive\\dokumenty\\documents\\skola\\matfyz\\phd\\prirodou-inspirovane-algoritmy\\02_zpetnovazebne_uceni\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opět si vytvoříme MountainCar prostředí. Můžeme si vypsat informace o rozměrech prostoru pozorování a akcí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "observation space low: [-1.2  -0.07]\n",
      "observation space high: [0.6  0.07]\n",
      "action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('observation space low:', env.observation_space.low)\n",
    "print('observation space high:', env.observation_space.high)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Před spuštěním simulace prostředí je potřeba ho resetovat a dostaneme se do prvního pozorování, tedy počátečního stavu s počáteční náhodnou pozicí blízko minima údolí a nulovou rychlostí. Potom je potřeba v prostředí udělat nějakou akci. Třeba opět náhodnou, která se nám koneckonců bude za chvíli hodit pro náhodného agenta. Tu uděláme pomocí metody ```sample()```. Samotný krok, tedy vykonání akce v prostředí a posun do dalšího stavu, uděláme pomocí metody ```step(action)``` a získáme nové pozorování, o kterém si můžeme vypsat další užitečné informace jako odměnu, informaci o konci simulace a další info.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation: [-0.58341146  0.        ]\n",
      "next observation: [-5.83965267e-01 -5.53808316e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použijeme nyní kód výše a vytvoříme si do něj třídu pro agenta, který se v prostředí zatím bude chovat náhodně, což nám sice v tuto chvíli nepřinese nic užitečného, ale můžete ho později použít jako základ pro zpětnovazebního agenta.\n",
    "\n",
    "Stav agenta dvojice je pozice a rychlost, akce může být pohyb vlevo, vpravo a nebo se nepohnout. Agent bude mít dvě metody, jednu na to, aby věděl, jak se má chovat a druhou aby se uměl resetovat. Agenta totiž budeme trénovat v několika iteracích. Pro zajímavost si také vypíšeme, jak vypadá prostředí, ve kterém se agent pohybuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obecna trida pro agenta\n",
    "class RandomAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.train = True\n",
    "    \n",
    "    def act(self, observe, reward, done):\n",
    "        return self.actions.sample()\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si napsat trénovací cyklus. Každá iterace for cyklu je jedna hra s novým náhodným začátkem (kolem minima). Ve while cyklu se trénují přechody mezi stavy agenta. Zároveň si pamatujeme celkovou odměnu a číslo kroku (jeden krok je provedení jedné akce), které se nám bude hodit pro logování trénovacích cyklu. K tomu, aby se agent mohl něco učit musí získávat od prostředí nějakou odměnu. V tomto příkladu by měl agent dostávat v každém kroku odměnu -1, když není v cílovém stavu a 0 pokud v něm je. Snižující se suma odměn totiž agenta nutí, aby prohledával prostředí, a tedy vyjel nahoru co nejdříve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(env.action_space)\n",
    "total_rewards = []\n",
    "for i in range(1000):\n",
    "    obs = env.reset()\n",
    "    agent.reset()    \n",
    "    done = False\n",
    "    \n",
    "    r = 0\n",
    "    R = 0 # celkova odmena - jen pro logovani\n",
    "    t = 0 # cislo kroku - jen pro logovani\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(obs, r, done)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "        \n",
    "    total_rewards.append(R)    \n",
    "agent.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na konec si zobrazíme animaci a graf učení, abychom viděli, jak se náš agent učil. Vidíme, že se moc neučil, což dává smysl, protože nemá implementované žádné rozumné učící se tělo (akce se vybírají náhodně). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_animation(agent, env, steps=1000, episodes=5)\n",
    "plt.plot(utils.moving_average(total_rewards, 10))\n",
    "plt.show() \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Úkol na cvičení\n",
    "\n",
    "Zkuste si místo náhodného agenta naprogramovat třídu agenta pomocí Q-učení, který se učí chovat v prostředí MountainCar. Dejte pozor na to, že prostředí vrací jako stav spojité hodnoty (poloha i rychlost jsou obě spojité), takže je třeba si z nich nějak udělat prostředí diskrétní (tedy s konečným množstvím stavů). Čím menší budou diskretizované intervaly, tím bude učení přesnější, ale tím déle bude trvat, takže je potřeba najít nějakou rozumnou hranici (ideálně vyzkoušením více hodnot). Dále můžete také experimentovat s dalšími parametry, například měnit maximální počty kroků, případně hodnotu odměny a pozorovat, jak se bude učení měnit.\n",
    "\n",
    "Při implementaci můžete vycházet z následujícího interface (ale nemusíte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateDiscretizer:\n",
    "  # predani rozmeru prostredi a spojitych stavu a jejich rozdeleni na diskretni intervaly\n",
    "    def __init__(self, ranges, states):\n",
    "        pass\n",
    "    \n",
    "    # prirazeni stavu do spravneho intervalu\n",
    "    def transform(self, obs):\n",
    "        pass\n",
    "        \n",
    "class QLearningAgent:\n",
    "    # nastaveni moznych akci - L, N, R   \n",
    "    # diskretizace stavu prostredi\n",
    "    # definice matice uzitku Q[stavy, akce]\n",
    "    # promenna na zapamatovani si minuleho stavu a minule akce\n",
    "    # donastaveni dalsich parametru trenovani\n",
    "    def __init__(self, actions, state_transformer, train=True):\n",
    "        pass\n",
    "    \n",
    "    # na zaklade stavu a akce se vybira nova akce\n",
    "    # 1. najde se nejlepsi akce pro dany stav\n",
    "    # 2. s malou pravd. vezme nahodnou\n",
    "    # 3. updatuje se Q matice\n",
    "    def act(self, observe, reward, done):\n",
    "        pass\n",
    "\n",
    "    # reset minuleho stavu a akce na konci epizody\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
